#!/usr/bin/env python
# -*- coding: utf8 -*-

'''Extract and clean data from OSCAR, mc4 and CC100

   You need to be logged in to HuggingFace via the CLI:

   huggingface-cli login'''

import sys
import argparse
from datasets import load_dataset

sys.stdin.reconfigure(encoding='utf-8')
sys.stdout.reconfigure(encoding='utf-8')


def create_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--corpus", type=str, required=True,
                        choices=["mc4", "oscar", "cc100", "MC4", "mC4", "OSCAR", "CC100"],
                        help="Which corpus to load")
    parser.add_argument("-o", "--output_file", type=str, required=True,
                        help="Output file to write to")
    parser.add_argument("-l", "--lang", type=str, required=True,
                        help="Language we want to select")
    args = parser.parse_args()
    return args


def process_simple(key, lang, output_file):
    '''Process the data that is in cc100 or mc4 standard'''
    if key.lower() == "mc4":
        dataset = load_dataset(key, lang, streaming=True, split="train", trust_remote_code=True)
    elif key.lower() == "cc100":
        dataset = load_dataset(key, lang=lang, streaming=True, split="train", trust_remote_code=True)
    docs = 0
    sents = 0
    dedup_dict = {}
    with open(output_file, "w", encoding="utf8") as out_f:
        for d in dataset:
            # Also already do exact deduplication here
            if d["text"].strip() and d["text"] not in dedup_dict:
                # Change newlines to space so we keep doc format as much as possible
                # Doesn't matter for LM training in our current setup, but could matter for annotation
                cur_sents = d["text"].strip().split("\n")
                sents += len(cur_sents)
                doc = " ".join(cur_sents).strip()
                out_f.write(doc + '\n')
                dedup_dict[doc] = 1
                docs += 1
    print (f"Found {sents} sents and {docs} docs in total")
    out_f.close()
    return docs


def process_oscar(lang, output_file):
    '''Process the data that is in OSCAR standard'''
    dataset = load_dataset("oscar-corpus/OSCAR-2301", token=True, language=lang,
                            streaming=True, split="train", trust_remote_code=True)
    num_docs, num_sents, actual_docs = 0, 0, 0
    dedup_dict = {}

    with open(output_file, "w", encoding="utf8") as out_f:
        for d in dataset:
            sents = d["text"].split('\n')
            idents = d["meta"]["sentence_identifications"]
            num_docs += 1
            full_doc = ""
            # If there are no quality warnings that's good
            # Otherwise only filter the document if it has the label "noisy"
            # Other warnings we keep, since they were calculated on the full-document level, and we
            # filter the individual sentences (footer/header) very likely anyway
            # It's also a matter of data: if we filter all documents with quality warnings
            # we only keep ~10% of all sentences
            if d["meta"]["quality_warnings"] == None or 'noisy' not in d["meta"]["quality_warnings"]:
                for idx, (sent, ident) in enumerate(zip(sents, idents)):
                    # Ignore all None sentences
                    if ident != None:
                        # Only take if correct language
                        if ident["label"] == lang:
                            full_doc += " " + sent.strip()
                            num_sents += 1

            # Add document on one line, also do exact deduplication
            if full_doc.strip() and full_doc not in dedup_dict:
                actual_docs += 1
                dedup_dict[full_doc.strip()] = 1
                out_f.write(full_doc.strip() + '\n')
    out_f.close()
    print (f"Found {num_sents} sents after {num_docs} docs ({actual_docs} actually included)")


def main():
    '''Main function'''
    args = create_arg_parser()

    # Load the data set
    if args.corpus.lower() == "oscar":
        process_oscar(args.lang, args.output_file)
    elif args.corpus.lower() in ["mc4", "cc100"]:
        process_simple(args.corpus, args.lang, args.output_file)


if __name__ == '__main__':
    # For logging purposes
    print("Generated by command:\npython", " ".join(sys.argv)+'\n')
    main()
